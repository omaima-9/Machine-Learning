{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkbMBbM539Z5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRyC4kbh39Z8"
      },
      "outputs": [],
      "source": [
        "def MyPerceptron (Data,N):\n",
        "    m,n=Data.shape\n",
        "    X=Data[:,0:n-1]\n",
        "    X=np.append(X,np.ones([m,1]),axis=1)\n",
        "    T=Data[:,n-1]\n",
        "    w=np.append(1,np.zeros([n-1,]))\n",
        "    L=np.ones([1,m])\n",
        "    Miss=[]\n",
        "    L=L[0]\n",
        "    L=L.tolist()\n",
        "    T=T.tolist()\n",
        "    for i in range (0,m):\n",
        "        if L[i] !=T[i]:\n",
        "            Miss.append(i)\n",
        "    Iter=0\n",
        "    while (np.size(Miss)!=0 and Iter<5000):\n",
        "        Iter=Iter+1\n",
        "        w=w+N*np.transpose(X[Miss[0],:])*T[Miss[0]]\n",
        "        Miss=[]\n",
        "        for i in range (0,m):\n",
        "            if np.matmul(np.transpose(w),np.transpose(X[i,:]))>0:\n",
        "                L[i]=1\n",
        "            else:\n",
        "                L[i]=-1\n",
        "            if L[i] !=T[i]:\n",
        "                if np.isin(i,Miss)==0:\n",
        "                    Miss.append(i)\n",
        "                    break\n",
        "    return w, Iter;\n",
        "         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBT6hZHh39Z8"
      },
      "outputs": [],
      "source": [
        "N=[1,10**-1,10**-2,10**-3,10**-4,10**-5,10**-6,10**-7,10**-8,10**-9];\n",
        "AccuracyAll=[];\n",
        "for n in range (0,10):\n",
        "    ## Step1: reading each image + appending to X array\n",
        "    X=[];\n",
        "    for i in range (1,2401):\n",
        "        img=np.array(Image.open('Assignment 2 Dataset/Train/'+str(i)+'.jpg'))\n",
        "        img=np.reshape(img,784)\n",
        "        img=img.astype(float)\n",
        "        X.append(img)\n",
        "    ## Step2: creating a weight vector for each digit + appending to WAll array\n",
        "    WAll=[]\n",
        "    for i in range (0,10):\n",
        "        T=-1*np.ones([240*10,1])\n",
        "        T[i*240:(i+1)*240]=1\n",
        "        s_train=np.append(X,T,axis=1)\n",
        "        w, Iter=MyPerceptron(s_train,N[n])\n",
        "        WAll.append(w)\n",
        "    ## Step3: using weight vectors on Test images to predict a class for each\n",
        "    C=[]\n",
        "    for i in range (1,201):\n",
        "        I = np.array(Image.open('Assignment 2 Dataset/Test/'+str(i)+'.jpg'))\n",
        "        XT=np.reshape(I,(1,784))\n",
        "        XT=XT.astype(float)\n",
        "        XT=np.append(XT,1)\n",
        "        PClass=[]\n",
        "        for j in range (0,10):\n",
        "            WTranspose=np.transpose(WAll[j])\n",
        "            P=np.matmul(WTranspose,XT)\n",
        "            PClass.append(P)\n",
        "        b=np.argmax(PClass)\n",
        "        C.append(b)\n",
        "    ## Step 4: creating a confusion matrix for all w of this n    \n",
        "    ConfusionMat=np.zeros([10,10])    \n",
        "    for i in range (0,10):\n",
        "        Ci=C[i*20:(i+1)*20]\n",
        "        for j in range (0,10):  \n",
        "            ConfusionMat[i,j]=Ci.count(j)\n",
        "    Accuracy=np.diag(ConfusionMat)\n",
        "    AccuracyAll.append(Accuracy)\n",
        "    MeanAccuracy=np.average(Accuracy)\n",
        "     \n",
        "print(Accuracy)        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LypQxPht39Z9"
      },
      "outputs": [],
      "source": [
        "print(Accuracy[0])   \n",
        "print(Accuracy[1])   \n",
        "print(Accuracy[2])   \n",
        "print(Accuracy[3])  \n",
        "print(Accuracy[4])   \n",
        "print(Accuracy[5])   \n",
        "print(Accuracy[6])   \n",
        "print(Accuracy[7]) \n",
        "print(Accuracy[8])   \n",
        "print(Accuracy[9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycSnGo_f39Z-"
      },
      "outputs": [],
      "source": [
        "# Random Forest Algorithm on Sonar Dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "from math import sqrt\n",
        "\n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "\tdataset = list()\n",
        "\twith open(filename, 'r') as file:\n",
        "\t\tcsv_reader = reader(file)\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\tif not row:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tdataset.append(row)\n",
        "\treturn dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = float(row[column].strip())\n",
        "\n",
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "\tclass_values = [row[column] for row in dataset]\n",
        "\tunique = set(class_values)\n",
        "\tlookup = dict()\n",
        "\tfor i, value in enumerate(unique):\n",
        "\t\tlookup[value] = i\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = lookup[row[column]]\n",
        "\treturn lookup\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor i in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores\n",
        "\n",
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right\n",
        "\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset, n_features):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfeatures = list()\n",
        "\twhile len(features) < n_features:\n",
        "\t\tindex = randrange(len(dataset[0])-1)\n",
        "\t\tif index not in features:\n",
        "\t\t\tfeatures.append(index)\n",
        "\tfor index in features:\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "\toutcomes = [row[-1] for row in group]\n",
        "\treturn max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, n_features, depth):\n",
        "\tleft, right = node['groups']\n",
        "\tdel(node['groups'])\n",
        "\t# check for a no split\n",
        "\tif not left or not right:\n",
        "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
        "\t\treturn\n",
        "\t# check for max depth\n",
        "\tif depth >= max_depth:\n",
        "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "\t\treturn\n",
        "\t# process left child\n",
        "\tif len(left) <= min_size:\n",
        "\t\tnode['left'] = to_terminal(left)\n",
        "\telse:\n",
        "\t\tnode['left'] = get_split(left, n_features)\n",
        "\t\tsplit(node['left'], max_depth, min_size, n_features, depth+1)\n",
        "\t# process right child\n",
        "\tif len(right) <= min_size:\n",
        "\t\tnode['right'] = to_terminal(right)\n",
        "\telse:\n",
        "\t\tnode['right'] = get_split(right, n_features)\n",
        "\t\tsplit(node['right'], max_depth, min_size, n_features, depth+1)\n",
        "\n",
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size, n_features):\n",
        "\troot = get_split(train, n_features)\n",
        "\tsplit(root, max_depth, min_size, n_features, 1)\n",
        "\treturn root\n",
        "\n",
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "\tif row[node['index']] < node['value']:\n",
        "\t\tif isinstance(node['left'], dict):\n",
        "\t\t\treturn predict(node['left'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['left']\n",
        "\telse:\n",
        "\t\tif isinstance(node['right'], dict):\n",
        "\t\t\treturn predict(node['right'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['right']\n",
        "\n",
        "# Create a random subsample from the dataset with replacement\n",
        "def subsample(dataset, ratio):\n",
        "\tsample = list()\n",
        "\tn_sample = round(len(dataset) * ratio)\n",
        "\twhile len(sample) < n_sample:\n",
        "\t\tindex = randrange(len(dataset))\n",
        "\t\tsample.append(dataset[index])\n",
        "\treturn sample\n",
        "\n",
        "# Make a prediction with a list of bagged trees\n",
        "def bagging_predict(trees, row):\n",
        "\tpredictions = [predict(tree, row) for tree in trees]\n",
        "\treturn max(set(predictions), key=predictions.count)\n",
        "\n",
        "# Random Forest Algorithm\n",
        "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
        "\ttrees = list()\n",
        "\tfor i in range(n_trees):\n",
        "\t\tsample = subsample(train, sample_size)\n",
        "\t\ttree = build_tree(sample, max_depth, min_size, n_features)\n",
        "\t\ttrees.append(tree)\n",
        "\tpredictions = [bagging_predict(trees, row) for row in test]\n",
        "\treturn(predictions)\n",
        "\n",
        "\n",
        "\n",
        "# Test the random forest algorithm\n",
        "seed(2)\n",
        "# load and prepare data\n",
        "filename = 'sonar.all-data.csv'\n",
        "dataset = load_csv(filename)\n",
        "# convert string attributes to integers\n",
        "for i in range(0, len(dataset[0])-1):\n",
        "\tstr_column_to_float(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "max_depth = 10\n",
        "min_size = 1\n",
        "sample_size = 1.0\n",
        "n_features = int(sqrt(len(dataset[0])-1))\n",
        "for n_trees in [1, 5, 10]:\n",
        "\tscores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
        "\tprint('Trees: %d' % n_trees)\n",
        "\tprint('Scores: %s' % scores)\n",
        "\tprint('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from .decision_tree import DecisionTree\n",
        "\n",
        "\n",
        "def bootstrap_sample(X, y):\n",
        "    n_samples = X.shape[0]\n",
        "    idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
        "    return X[idxs], y[idxs]\n",
        "\n",
        "\n",
        "def most_common_label(y):\n",
        "    counter = Counter(y)\n",
        "    most_common = counter.most_common(1)[0][0]\n",
        "    return most_common\n",
        "\n",
        "\n",
        "class RandomForest:\n",
        "    def __init__(self, n_trees=10, min_samples_split=2, max_depth=100, n_feats=None):\n",
        "        self.n_trees = n_trees\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.n_feats = n_feats\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTree(\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                max_depth=self.max_depth,\n",
        "                n_feats=self.n_feats,\n",
        "            )\n",
        "            X_samp, y_samp = bootstrap_sample(X, y)\n",
        "            tree.fit(X_samp, y_samp)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
        "        y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "\n",
        "# Testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Imports\n",
        "    from sklearn import datasets\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    def accuracy(y_true, y_pred):\n",
        "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "        return accuracy\n",
        "\n",
        "    data = datasets.load_breast_cancer()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=1234\n",
        "    )\n",
        "\n",
        "    clf = RandomForest(n_trees=3, max_depth=10)\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    acc = accuracy(y_test, y_pred)\n",
        "\n",
        "    print(\"Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from collections import Counter\n",
        "\n",
        "import random\n",
        "random.seed(10)\n",
        "\n",
        "def subsample(x,y, n_sample):\n",
        "    samplex = list()\n",
        "    sampley = list()\n",
        "    while len(samplex) < n_sample:\n",
        "        index = randrange(len(x))\n",
        "        samplex.append(x[index])\n",
        "        sampley.append(y[index])\n",
        "\n",
        "    return samplex,sampley\n",
        "\n",
        "\n",
        "def bagging_predict(trees, row):\n",
        "    predictions = [predict(tree, row) for tree in trees]\n",
        "    return max(set(predictions), key=predictions.count)\n",
        "\n",
        "def most_common_label(y):\n",
        "    counter = Counter(y)\n",
        "    most_common = counter.most_common(1)[0][0]\n",
        "    return most_common\n",
        "\n",
        "def bootstrap_sample(X, y,n_samples):\n",
        "    idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
        "    for i in range(len(idxs)):\n",
        "        X[i]=X[idxs[i]]\n",
        "        y[i]=y[idxs[i]]\n",
        "    return X, y \n",
        "\n",
        "# def predict(self, X):\n",
        "#     tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "#     tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
        "#     y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n",
        "#     return np.array(y_pred)\n",
        "\n",
        "def RandomForest(X,y,NumOfTrees,MaxDepth,NumOfSamples):\n",
        "    trees = []\n",
        "    for i in range(NumOfTrees):\n",
        "#         X_samp, y_samp = bootstrap_sample(X, y,NumOfSamples)\n",
        "        X_samp, y_samp = subsample(X,y, NumOfSamples)\n",
        "        tree = DecisionTreeClassifier(max_depth=MaxDepth,random_state=10)\n",
        "        tree.fit(X_samp, y_samp)\n",
        "        trees.append(tree)\n",
        "        predictions = [bagging_predict(trees, row) for row in y]\n",
        "        return(predictions)\n",
        "#         print (trees[i])\n",
        "#         tree_preds = np.array([tree.predict(X) for tree in trees])\n",
        "#         tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
        "#         y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n",
        "#         return(y_pred)\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Assignment2Solution.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b667cebad148e7b094a58ee81f940c685de1dd70a003a9ccdca4a5792431bee5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
